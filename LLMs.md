---
title: "Introduction to </br> Digital Humanities"
subtitle: "(Large Language Models -- LLMs)"
author: Maciej Eder
date: 2025/10/17
format: 
  revealjs:
    katex: true
    theme: files/pp.scss
    logo: files/DigiTS-RGB-short-SMALL.svg
    css: files/big_logo_DigiTS.css
    self_contained: false
---



## Overview

- What are LLMs?
- Why they matter for Digital Humanities (DH)
- Key concepts & terminology
- Architecture & training pipeline
- Common models & useâ€‘cases
- Ethics, bias & responsible use
- Quick handsâ€‘on demo
- Resources & next steps

---

## What is a Language Model?

- **Definition** â€“ A statistical model that predicts the next word (or
token) in a sequence.
- **Goal** â€“ Capture the structure, style, and content of natural
language.
- **From nâ€‘gram to neural**
  - *nâ€‘gram*: countâ€‘based, limited context
  - *Neural*: learn continuous representations, longâ€‘range dependencies

> *Speaker note:* Emphasise that the â€œlanguage modelâ€ is the core of many
NLP systems: translation, summarisation, chat, etc.

---

## Why LLMs Matter for DH

| DH Task | How LLMs Help |
|---------|---------------|
| **Textual Analysis** | Generate summaries, sentiment, topic models |
| **Digital Archiving** | Automate metadata extraction, OCR postâ€‘processing |
| **Translation & Localization** | Bridge language gaps in primary sources |
| **Creative Projects** | Coâ€‘authoring, generating prompts for digital art |
| **Interoperability** | Convert legacy formats, standardise vocabularies |

*Illustration idea:* Show a workflow diagram: *Source Text â†’ LLM â†’
Enriched Metadata*

---

## Key Concepts & Terminology

| Term | Simple Explanation |
|------|--------------------|
| **Token** | A unit of text (word, subâ€‘word, punctuation). |
| **Embedding** | Dense vector representation of a token. |
| **Attention** | Mechanism that weighs the influence of tokens on each other. |
| **Selfâ€‘Attention** | Tokens attend to all tokens in the same sequence. |
| **Parameter** | Numerical weight learned during training (e.g., 175B for GPTâ€‘3). |
| **Prompt** | Input text that guides the modelâ€™s output. |
| **Fineâ€‘tuning** | Further training on a specific domain or task. |
| **Zeroâ€‘shot / Fewâ€‘shot** | Using the model without, or with minimal, taskâ€‘specific examples. |

---

## The Training Pipeline

1. **Data Collection**
   - Web scrapes, books, Wikipedia, news, code, etc.
2. **Tokenisation**
   - Byteâ€‘pair encoding (BPE) or WordPiece â†’ subâ€‘word units.
3. **Preâ€‘training Objective**
   - *Masked Language Modelling* (BERT style)
   - *Causal Language Modelling* (GPT style)
4. **Optimization**
   - Gradient descent, Adam optimiser, largeâ€‘batch training.
5. **Evaluation**
   - Perplexity on heldâ€‘out data, benchmark tests (GLUE, SuperGLUE).


---

## Transformer Architecture

- **Encoderâ€‘decoder vs. Decoderâ€‘only**
  - GPT, LLaMA = decoderâ€‘only
  - BERT = encoderâ€‘only
- **Selfâ€‘Attention Formula**

$$ \text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^{\top}}{\sqrt{d_k}}\right)V $$

- **Layer Composition**
  1. Multiâ€‘head selfâ€‘attention
  2. Add & Norm (residual connection)
  3. Positionâ€‘wise Feedâ€‘Forward (dense layers)
  4. Add & Norm

- **Positional Encoding**
  - Adds sequence order information (sin/cos or learned).

> *Illustration idea:* Show a block diagram of a single transformer layer.


---

## Popular LLMs & Their Origins

| Model | Release | Parameter Count | Training Data | Key Features |
|-------|---------|-----------------|---------------|--------------|
| **GPTâ€‘4** | 2023 | 1â€¯trillion+ (approx.) | Web + proprietary | Multimodal, lowâ€‘promptâ€‘overhead |
| **Claude 2** | 2023 | ~52B | Anthropic curated | Safetyâ€‘first design |
| **LLaMA 2** | 2023 | 7B, 13B, 70B | Metaâ€‘public corpora | Openâ€‘source, fineâ€‘tune friendly |
| **Mistral 7B** | 2023 | 7B | Public data | Efficiency, lowâ€‘latency |
| **BERT** | 2018 | 110M | Wikipedia + BookCorpus | Contextual embeddings (bidirectional) |

> *Speaker note:* Emphasise openâ€‘source models (LLaMA, Mistral) give
students a chance to run locally.

---

## How LLMs Work in Practice

```
User prompt  -->  Tokenise  -->  Model Forward Pass  -->  Generate Tokens
-->  Detokenise  -->  Output
```

- **Prompt Engineering** â€“ Style, length, and context shape the answer.
- **Temperature** â€“ Controls randomness (0.1 = deterministic, 1.0 =
creative).
- **Topâ€‘k / Topâ€‘p (nucleus)** â€“ Sampling strategies.

> *Interactive demo idea:* Show live generation of a short poem about â€œThe
Library of Alexandriaâ€.

---

## Use Cases in Digital Humanities

| Task | Example | Model/Tool | Comments |
|------|---------|------------|----------|
| **Automated Cataloguing** | Extract author, title, date from scanned manuscripts | GPTâ€‘4 via prompt | Speed up metadata entry |
| **Textual Commentary** | Generate footnotes for archaic phrases | LLaMA 2 fineâ€‘tuned on critical editions | Requires domain fineâ€‘tuning |
| **Historical Text Summaries** | Summarise newspaper articles | Claude 2, prompt with â€œSummarise in 3 sentencesâ€ | Helps readers skim |
| **Digital Art Prompting** | Create image prompts for DALLâ€‘E from literary descriptions | GPTâ€‘4 or Claude | Bridges literature & visual arts |
| **Code Generation** | Convert OCR errors to clean XML | GPTâ€‘4 codeâ€‘generation | Reduce manual postâ€‘processing |
| **Questionâ€‘Answering** | Answer queries about Shakespeareâ€™s plays | BERT fineâ€‘tuned on TEI XML | Interactive knowledge base |

---

## Ethical Considerations

| Issue | What it means for DH | Mitigation |
|-------|----------------------|------------|
| **Bias & Stereotype Amplification** | Misrepresentation of minority groups in generated text | Use diverse training data; postâ€‘filtering |
| **Plagiarism & Attribution** | AI can produce text too close to sources | Check originality; attribute AI usage |
| **Data Privacy** | Models trained on personal data (e.g., letters) | Use deâ€‘identified corpora; comply with GDPR |
| **Algorithmic Transparency** | Blackâ€‘box decisions in summarisation | Explainable prompts; human oversight |
| **Digital Divide** | Access to large GPU resources | Leverage openâ€‘source, distilled models |

> *Speaker note:* Encourage students to think critically about â€œownershipâ€
of AIâ€‘generated content, especially when publishing in DH projects.


---

## Handsâ€‘On Miniâ€‘Demo

1. **Tool** â€“ ChatGPT (free tier) or openâ€‘source LLaMA 2 on Colab.
2. **Prompt** â€“ â€œGenerate a concise metadata record for the following
manuscript excerpt: â€¦â€
3. **Show** â€“ Compare model output vs. humanâ€‘written record.
4. **Discussion** â€“ Accuracy, missing fields, style.



---

## Limitations & Future Directions

- **Scalability** â€“ Training requires petaflopâ€‘days; inference latency can
be high.
- **Hallucinations** â€“ AI may fabricate facts; factâ€‘checking is essential.
- **Interpretability** â€“ Understanding why a model makes a decision is
still hard.
- **Multimodality** â€“ Integration of text, images, audio opens new DH
possibilities.
- **Continual Learning** â€“ Models that adapt to new corpora without
forgetting.
- **Openâ€‘source momentum** â€“ More accessible LLMs (e.g., LLaMA 2, Mistral)
democratise research.

---

## Summary

- LLMs are powerful tools that can transform DH workflows.
- Core ideas: tokenisation, embeddings, selfâ€‘attention, transformer
layers.
- Training on massive corpora yields broad knowledge but introduces bias.
- Practical use requires careful prompt design, evaluation, and ethical
vigilance.
- The field is rapidly evolving; staying current with literature & tools
is key.

---

## Further Reading & Resources

| Resource | Link |
|----------|------|
| *Attention Is All You Need* (Vaswani et al., 2017) | https://arxiv.org/abs/1706.03762 |
| *Language Models are Few-Shot Learners* (Brown et al., 2020) | https://arxiv.org/abs/2005.14165 |
| *LLaMA 2* (Meta) | https://github.com/facebookresearch/llama/tree/main |
| *Mistral* (Mistral AI) | https://github.com/mistralai/mistral |
| *OpenAI API Documentation* | https://platform.openai.com/docs |
| *Hugging Face Spaces* | https://huggingface.co/spaces |
| *Digital Humanities Association* | https://dhan.org |

---

## Thank You!

Questions?

ğŸ”— Connect on Twitter: @YourUniDH

ğŸ“§ Email: dhdept@university.edu


# (generated by an AI model!!!)


